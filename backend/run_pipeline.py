# run_pipeline.py
#!/usr/bin/env python3
"""
Complete ML Pipeline Runner
Runs all steps from data loading to deployment with enhanced metrics
"""

import subprocess
import sys
import os
import json

def run_step(step_num, step_name, python_script):
    """Run a Python script for a pipeline step"""
    print(f"\n{step_num} {step_name}")
    print("-" * 40)
    
    try:
        # Run the Python script
        result = subprocess.run(
            ["python3", "-c", python_script],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print("‚úÖ Success!")
            if result.stdout:
                # Print relevant output
                lines = result.stdout.split('\n')
                for line in lines:
                    if any(keyword in line for keyword in ['Accuracy:', 'Precision:', 'Recall:', 'F1-Score:', '‚úÖ', '‚ùå', '‚ö†Ô∏è']):
                        print(f"  {line}")
        else:
            print("‚ùå Failed!")
            if result.stderr:
                print("Error:", result.stderr[:500])
        return result.returncode == 0
    except Exception as e:
        print(f"‚ùå Exception: {e}")
        return False

def display_summary():
    """Display training summary from metrics file"""
    metrics_file = "ml_model/evaluation_results/detailed_metrics.json"
    
    if os.path.exists(metrics_file):
        print("\n" + "="*60)
        print("üìä TRAINING SUMMARY")
        print("="*60)
        
        try:
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
            
            print(f"\nü§ñ Model: {metrics['model_name']}")
            print(f"üìÖ Trained: {metrics['timestamp']}")
            print(f"üß™ Test Samples: {metrics['test_samples']}")
            print(f"üèãÔ∏è‚Äç‚ôÇÔ∏è Train Samples: {metrics['train_samples']}")
            print(f"üéØ Features Used: {len(metrics['features_used'])}")
            
            print("\nüìà Overall Performance:")
            overall = metrics['overall_metrics']
            print(f"   ‚Ä¢ Accuracy:  {overall['accuracy']:.4f}")
            print(f"   ‚Ä¢ Precision: {overall['weighted_precision']:.4f}")
            print(f"   ‚Ä¢ Recall:    {overall['weighted_recall']:.4f}")
            print(f"   ‚Ä¢ F1-Score:  {overall['weighted_f1']:.4f}")
            
            print("\nüéØ Per-Class Performance:")
            for class_name, class_metrics in metrics['per_class_metrics'].items():
                status = "‚úÖ" if class_metrics['f1_score'] > 0.8 else "‚ö†Ô∏è" if class_metrics['f1_score'] > 0.6 else "‚ùå"
                print(f"   {status} {class_name}: P={class_metrics['precision']:.3f}, R={class_metrics['recall']:.3f}, F1={class_metrics['f1_score']:.3f}")
            
            print("\nüìÅ Generated Files:")
            files = [
                "model_report.html",
                "detailed_metrics.json",
                "confusion_matrix.png",
                "feature_importance.png",
                "precision_recall_plot.png"
            ]
            
            for file in files:
                path = f"ml_model/evaluation_results/{file}"
                if os.path.exists(path):
                    print(f"   ‚Ä¢ {file}")
                else:
                    print(f"   ‚Ä¢ {file} (missing)")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load metrics: {e}")

def main():
    print("=" * 60)
    print("üöÄ CREDIT SCORE AI - COMPLETE ML PIPELINE")
    print("=" * 60)
    
    # Check if in virtual environment
    if not os.path.exists("venv") and not os.path.exists("../venv"):
        print("\n‚ö†Ô∏è  Virtual environment not found!")
        print("Setting up virtual environment first...")
        
        # Create virtual environment
        subprocess.run(["python3", "-m", "venv", "venv"])
        print("‚úÖ Virtual environment created")
        
        # Activate and install requirements
        print("Installing requirements...")
        if sys.platform == "darwin" or sys.platform == "linux":
            subprocess.run(["./venv/bin/pip", "install", "-r", "requirements.txt"])
        else:
            subprocess.run(["venv\\Scripts\\pip", "install", "-r", "requirements.txt"])
    
    # Step 1: Data Loading & Cleaning
    step1_code = """
import sys
import os
sys.path.append('.')
from ml_model.data_processor import DataProcessor
p = DataProcessor('CIBIL_Credit_Score_Large_Dataset.csv')
df = p.load_data()
df = p.clean_data()
print(f"‚úÖ Cleaned data shape: {df.shape}")
"""
    
    # Step 2: EDA
    step2_code = """
import sys
import os
sys.path.append('.')
from ml_model.data_processor import DataProcessor
p = DataProcessor('CIBIL_Credit_Score_Large_Dataset.csv')
p.load_data()
p.clean_data()
p.exploratory_analysis()
print("‚úÖ EDA completed. Check 'eda_results/' folder")
"""
    
    # Step 3: Model Training with Enhanced Metrics
    step3_code = """
import sys
import os
sys.path.append('.')
from ml_model.train_model import train_credit_score_model
model, scaler, encoder, name, acc = train_credit_score_model()
if model:
    print(f"‚úÖ Model trained: {name}")
    print(f"üìä Check ml_model/evaluation_results/ for detailed metrics")
"""
    
    # Run steps
    steps = [
        ("1Ô∏è‚É£", "Data Loading & Cleaning", step1_code),
        ("2Ô∏è‚É£", "Exploratory Data Analysis", step2_code),
        ("3Ô∏è‚É£", "Model Training & Evaluation", step3_code),
    ]
    
    for step_num, step_name, step_code in steps:
        success = run_step(step_num, step_name, step_code)
        if not success:
            print(f"\n‚ùå Pipeline stopped at {step_name}")
            return
    
    # Display summary
    display_summary()
    
    # Step 4: API Server
    print("\n" + "="*60)
    print("4Ô∏è‚É£ Start API Server")
    print("="*60)
    
    print("\nüåê API Endpoints Available:")
    print("   ‚Ä¢ GET  /              - Welcome message")
    print("   ‚Ä¢ GET  /health        - Health check")
    print("   ‚Ä¢ POST /predict       - Make predictions")
    print("   ‚Ä¢ GET  /docs          - Interactive API docs (Swagger UI)")
    print("   ‚Ä¢ GET  /redoc         - Alternative API docs")
    
    print("\nüìÅ Check your model report:")
    print("   file://" + os.path.abspath("ml_model/evaluation_results/model_report.html"))
    
    choice = input("\nüöÄ Start API server? (y/n): ")
    if choice.lower() == 'y':
        print("\n" + "="*60)
        print("üöÄ Starting API server on http://localhost:8000")
        print("üìö Open http://localhost:8000/docs for API documentation")
        print("üõë Press Ctrl+C to stop")
        print("="*60)
        subprocess.run(["python3", "run.py"])
    else:
        print("\n‚úÖ Pipeline completed successfully!")
        print("\nüìã Next steps:")
        print("   1. Start API server: python run.py")
        print("   2. Test API: python test_api.py")
        print("   3. View report: open ml_model/evaluation_results/model_report.html")

if __name__ == "__main__":
    main()